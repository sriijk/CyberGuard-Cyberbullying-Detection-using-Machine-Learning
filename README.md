# ğŸ›¡ï¸ CyberGuard â€“ Cyberbullying Detection using Machine Learning

**CyberGuard** is an ML-powered web application designed to detect cyberbullying in social media posts. It leverages natural language processing (NLP) with logistic regression to classify harmful and non-harmful text. The goal is to assist in creating safer and more respectful online spaces.

 CyberGuard offers a practical and extendable framework for identifying harmful content using machine learning. While currently limited in scope, it sets a strong foundation for further improvements and real-world deployment.
 
---

## ğŸ” Features

- Text classification using TF-IDF + Logistic Regression
- Detects hate speech, profanity, threats, and verbal abuse
- Clean and informative web interface (Flask + HTML)
- Graphs and metrics to understand model performance
- Modular code for model training and UI rendering

---

## ğŸ› ï¸ Tech Stack

| Component      | Technology                                  |
|----------------|----------------------------------------------|
| Language       | Python 3                                     |
| ML Libraries   | Scikit-learn, Pandas, Seaborn                |
| NLP Technique  | TF-IDF Vectorization                         |
| Classifier     | Logistic Regression                          |
| Backend        | Flask (also used for rendering frontend)     |
| Frontend       | HTML (rendered via Flask + Jinja2)           |
| Visualization  | Matplotlib, Seaborn                          |


---

## ğŸš€ How It Works

1. **Input text** is collected from the user via the web UI.
2. Text is transformed using a **pre-trained TF-IDF vectorizer**.
3. A **trained Logistic Regression model** predicts whether the text is cyberbullying or not.
4. Prediction is returned and displayed instantly on the interface.
5. The model was trained on a real dataset containing offensive and non-offensive online text.

---

## ğŸ§ª Model Evaluation Summary

| Metric      | Score    |
|-------------|----------|
| Accuracy    | 94.2%    |
| Precision   | 92.7%    |
| Recall      | 93.1%    |
| F1-Score    | 92.9%    |

> These are sample values. Replace with your actual model results if available.

---

## ğŸ“Š Graphical Analysis

Hereâ€™s how the model performed during evaluation:

| âœ… Accuracy Score | ğŸ“‰ Confusion Matrix |
|------------------|---------------------|
| ![Accuracy Score](accuracy_score.png) | ![Confusion Matrix](confusion_matrix.png) |

| ğŸ§® Classification Metrics | ğŸ§¬ ROC Curve |
|---------------------------|--------------|
| ![Classification Metrics](heatmap.png) | ![ROC Curve](ROCgraph.png) |

--- 

## ğŸ’» How to Run Locally

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/CyberGuard.git
   cd CyberGuard
2. **Create virtual environment**
   ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate

3. **Install dependencies**
   ```bash
    pip install -r requirements.txt

4. **Run the Flask app**
   ```bash
    python app.py
   
5. **Open your browser and visit**
   ```cpp
    http://127.0.0.1:5000/

---

## ğŸ§  Dataset

This project uses a labeled dataset of social media posts/texts annotated as offensive or non-offensive.

- ğŸ—‚ **Text Type:** Short social media comments/posts
- ğŸ· **Labels:** Cyberbullying (1), Non-cyberbullying (0)
- ğŸ“Š **Size:** ~10,000 entries (or mention actual count)
- ğŸ“š **Source:** (e.g., Kaggle, manually created, etc.)

Note: Dataset not included in the repository due to licensing. You can use any binary classification dataset for training.

---

## ğŸ“ Project Structure

```
CyberGuard/
â”œâ”€â”€ app.py                      # Flask web app
â”œâ”€â”€ train_model.py              # Model training script
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ cyberbully_model.pkl
â”‚   â”œâ”€â”€ vectorizer.pkl
â”‚   â”œâ”€â”€ accuracy_score.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ classification_metrics_heatmap.png
â”‚   â””â”€â”€ roc_curve.png
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ home.html
â”‚   â”œâ”€â”€ detect.html
â”‚   â”œâ”€â”€ about.html
â”‚   â”œâ”€â”€ services.html
â”‚   â””â”€â”€ contact.html
â”œâ”€â”€ static/
â”‚   â””â”€â”€ (optional CSS/images if added later)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

---

## ğŸ“Œ Use Cases

- ğŸ§‘â€ğŸ« **For Educators:** Identify online abuse among students to foster safer school environments.
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ **For Parents:** Monitor and detect harmful content to protect children online.
- ğŸ§‘â€ğŸ’¼ **For Moderators:** Aid in identifying harmful comments for community moderation.
- ğŸ§ª **For Researchers/Students:** Extend the model with other datasets or classifiers for academic purposes.

---

## ğŸ”„ Future Scope

The CyberGuard system lays a strong foundation, but there's immense potential for improvement and real-world deployment. Future developments can include:

- ğŸ“ˆ **Larger Dataset:** Incorporate more diverse and real-world samples to boost model accuracy, robustness, and generalization.
- ğŸ§  **Advanced Models:** Implement deep learning techniques such as **BERT**, **LSTM**, or **transformer-based architectures** for improved contextual understanding of text.
- ğŸ§© **Multi-Class Classification:** Go beyond binary classification to detect different categories of harmful content like hate speech, threats, profanity, and sarcasm.
- ğŸ“± **Real-Time Deployment:** Convert the application into a fully functional **web or mobile platform** capable of live monitoring and detection of cyberbullying.

---

## ğŸ‘¨â€ğŸ’» Developed By

**Srishti Bhatnagar**  
Linkedin: [Srishti Bhatnagar](www.linkedin.com/in/srishti-bhatnagar-b59833269)  
